#!/usr/bin/env python
# -*- coding: utf-8 -*-  
# by zhangzhi @2014-08-24 21:07:29 
# Copyright 2014 NONE rights reserved.


from setup import iPapa
from bs4 import BeautifulSoup as BS
import os
import urlparse
from iTask import Task
import util
import re
import logging

class SzTableParser(object):
    def parse(self, task):
        print "ContentPageHandler parse", task.url, task['key']
        newTasks = []
        ret, status = self.parseContent(task['__data'])
        meta = {}
        if status == 'OK':
            key = task['key']
            keyOutputPath = os.path.join(iPapa.iTsOutputPath, key)
            #siteTile 
            meta['siteTitle'] = ret['siteTitle']
            #title
            meta['title'] = ret['title']
            # url
            meta['url'] = task.url
            # date
            meta['date'] = ret['date']
            #contentPics
            #record and new task to download it
            meta['contentPics'] = ret['contentPics']
            meta['contentPicCaptions'] = ret['contentPicCaptions']
            meta['embPics'] = ret['embPics']
            #create new tasks here
            if len(ret['contentPics']) and len(ret['contentPicCaptions']):
                picUrl = ret['contentPics'][0]
                #only the first pic here, ignore others now
                dest = os.path.join(key, util.getUrlFileName(picUrl)) 
                newT = Task(-1, url=picUrl, handler='PicHandler', taskType='media', ref=task.url, dest=dest)  
                newT['key'] = task['key']
                newT['picType'] = 'contentPic'
                newTasks.append(newT)

            #for content, we store it 
            contentLoc = os.path.join(keyOutputPath, 'content.html')
            util.writeFile(contentLoc, ret['content'])
            # contentMp3 
            if 'contentMp3' in ret:
                url = ret['contentMp3']
                dest = os.path.join(key, util.getUrlFileName(url)) 
                newT = Task(-1, url=url, handler='AudioHandler', taskType='media', ref=task.url, dest=dest)  
                newT['key'] = task['key']
                newT['audioType'] = os.path.splitext(dest)[1].upper()
                newTasks.append(newT)
            elif 'contentMp3Page' in ret: #always be with big file, we ignore it 
                #url = ret['contentMp3Page']
                #newT = Task(-1, url=urlparse.urljoin(task.url, url), handler='ContentMp3PageHandler', ref=task.url,) 
                #newT['key'] = task['key']
                #newTasks.append(newT)
                task.status = 'ignore' 
                meta['isIgnore'] = True
                meta['ignoreMsg'] = "Audio file is too big, we should ignore this now."
                task.msg = 'Audio file is too big, we should ignore this now.' 
            else:
                #Failed
                task.status = 'failed' 
                task.msg = 'failed in Findding a Audio' 

            # download here 
            # embPics
            for embPic in ret['embPics']:
                picUrl = embPic
                #only the first pic here, ignore others now
                dest = os.path.join(key, util.getUrlFileName(picUrl)) 
                newT = Task(-1, url=picUrl, handler='PicHandler', taskType='media', ref=task.url, dest=dest)  
                newT['key'] = task['key']
                newT['picType'] = 'embPic'
                newTasks.append(newT)

            # store meta file
            metaLoc = os.path.join(keyOutputPath, 'meta.json') 
            if util.dump2JsonFile(meta, metaLoc) != True:
                task.status = 'failed'    

        else:
            task.status = 'failed'
        if task.status == 'ignore': 
            return {}
        if newTasks != []:
            return {'newTasks': newTasks}
        return {}

    def parseContent(self, page):
        transMapCn2En = { \
            '公司代码': 'cmpCode',
            '公司简称': 'compNameAbbr',
            '英文名称': 'compNameEn',
            '注册地址': 'regAddr',
            'A股代码' : 'code',
            'A股简称' : 'nameCn',
            'A股上市日期': 'A_IPO_date',
            'A股总股本': 'A_totalShares',
            'A股流通股本':'A_circulationStock',
            'B股代码': 'B_code',
            'B股简称': 'B_code',
            'B股上市日期': 'B_IPO_date',
            'B股总股本': 'B_totalShares',
            'B股流通股本': 'B_circulationStock',
            '地区': 'region',
            '省份': 'province',
            '城市': 'city',
            '所属行业': 'sector',
            '公司网址': 'webSite'
        }
        transMapEn2Cn = {}
        for k in transMapCn2En:
            transMapEn2Cn[transMapCn2En[k]] = k

        ret = {'stockList':[], 'headerList':[]}
        try:
            soup = BS(page)           
            table = soup.find('table')
            trList  = table.find_all('tr')
            trHeader = trList[0]

            thList = trHeader.find_all('td')
            headerList = [ td.text.strip() for td in thList ]
            ret['headerList'] = headerList

            for tr in trList[1:]: # for each stock
                tdList = tr.find_all('td')
                tdTextList = [ td.text for td in tdList ]
                
                cmpCode = tdTextList[transMapEn2Cn['cmpCode']]
                for i in len(tdTextList):
                    = tdTextList[i]        
            
        except Exception, e:
            util.printException()
            return (None, e)

        return (ret, 'OK')


if __name__ == '__main__':
    import sys
    inputCase = sys.argv[1]
    data = open(inputCase).read()
    m = SzTableParser()
    ret, status =  m.parseContent(data.decode('gbk'))

